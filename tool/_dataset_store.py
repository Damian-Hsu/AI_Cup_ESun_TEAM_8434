import os
import json
import shutil
from pathlib import Path
from datetime import datetime
import hashlib
from typing import Optional, List, Dict, Literal

import pandas as pd


class DatasetStore:
    """
    簡易資料版本倉庫：
    root
    └── dataset
        └── <name>
            └── <version>
                ├── data.parquet / data.csv
                └── meta.json
    """
    def __init__(self, root: str = "./.data"):
        """
        Initialize the dataset version store under the given root directory.

        This constructor ensures that the root `dataset` directory exists so that
        subsequent save/load operations can create and read versioned datasets
        under this path.

        Parameters
        ----------
        root : str, optional
            Base directory in which the `dataset/` folder will be created.
            Defaults to "./.data".
        """
        self.root = Path(root)
        (self.root / "dataset").mkdir(parents=True, exist_ok=True)

    # ---------- 路徑工具 ----------
    def _ds_dir(self, name: str) -> Path:
        """
        Return the directory path for a given dataset name.

        Parameters
        ----------
        name : str
            Logical name of the dataset (e.g. "train_txn", "graph_features").

        Returns
        -------
        Path
            Path object pointing to `<root>/dataset/<name>`.
        """

        return self.root / "dataset" / name

    def _ver_dir(self, name: str, version: str) -> Path:
        """
        Return the directory path for a specific dataset version.

        Parameters
        ----------
        name : str
            Dataset name.
        version : str
            Version identifier (usually a timestamp string generated by `_now_ver`).

        Returns
        -------
        Path
            Path object pointing to `<root>/dataset/<name>/<version>`.
        """

        return self._ds_dir(name) / version

    def _now_ver(self) -> str:
        """
        Generate a new version string based on the current timestamp.

        The format is `YYYYMMDD-HHMMSS-mmm`, where `mmm` is milliseconds, so that
        lexicographic ordering of versions matches chronological order.

        Returns
        -------
        str
            A timestamp-based version identifier.
        """

        return datetime.now().strftime("%Y%m%d-%H%M%S-%f")[:-3]
    
    def _hash_df(self, df: pd.DataFrame) -> str:
        """計算 DataFrame 的 hash，用於內容比對"""
        return hashlib.sha256(
            pd.util.hash_pandas_object(df, index=True).values.tobytes()
        ).hexdigest()
    def _latest_version(self, name: str) -> Optional[Path]:
        """
        Get the latest (most recent) version directory for a given dataset.

        This inspects all subdirectories under `<root>/dataset/<name>` and returns
        the one with the greatest lexicographic order, which corresponds to the
        newest version generated by `_now_ver`.

        Parameters
        ----------
        name : str
            Dataset name.

        Returns
        -------
        Path or None
            Path to the latest version directory, or ``None`` if no versions exist.
        """

        d = self._ds_dir(name)
        if not d.exists():
            return None
        # 只取資料夾並排序（字串排序即可對應你的時間戳版本命名）
        vers = sorted([p for p in d.iterdir() if p.is_dir()])
        return vers[-1] if vers else None
    

    # ---------- 版本列出 ----------
    def list_versions(self, name: str) -> List[str]:
        """
        List all existing versions for a given dataset.

        The returned version names are sorted lexicographically, which corresponds
        to chronological order if versions were created using `_now_ver`.

        Parameters
        ----------
        name : str
            Dataset name.

        Returns
        -------
        List[str]
            Sorted list of version identifiers for the dataset. Returns an empty
            list if the dataset has no versions yet.
        """

        d = self._ds_dir(name)
        if not d.exists():
            return []
        versions = [p.name for p in d.iterdir() if p.is_dir()]
        versions.sort()  # 字串排序即可符合時間
        return versions

    # ---------- 讀取 ----------
    def load(self, name: str, version: str = "latest") -> pd.DataFrame:
        """
        Load a dataset version as a pandas DataFrame.

        The method will:
        1. Resolve "latest" to the newest available version, if needed.
        2. Prefer reading `data.parquet` when present.
        3. Fall back to `data.csv` if parquet does not exist.
        If neither file is found, a FileNotFoundError is raised.

        Parameters
        ----------
        name : str
            Dataset name to load.
        version : str, optional
            Version identifier to load. Use "latest" (default) to automatically
            pick the newest version.

        Returns
        -------
        pd.DataFrame
            The loaded dataset.

        Raises
        ------
        FileNotFoundError
            If no versions exist, or the requested version does not exist, or
            no data file is present in that version directory.
        """

        if version == "latest":
            versions = self.list_versions(name)
            if not versions:
                raise FileNotFoundError(f"No versions found for dataset '{name}'.")
            version = versions[-1]

        vdir = self._ver_dir(name, version)
        if not vdir.exists():
            raise FileNotFoundError(f"Version '{version}' not found for '{name}'.")

        # 優先讀 parquet，其次 csv
        f_parquet = vdir / "data.parquet"
        f_csv = vdir / "data.csv"
        if f_parquet.exists():
            return pd.read_parquet(f_parquet)
        elif f_csv.exists():
            return pd.read_csv(f_csv)
        else:
            raise FileNotFoundError(f"No data file in version '{version}' of '{name}'.")

    def load_meta(self, name: str, version: str = "latest") -> Dict:
        """
        Load the metadata dictionary for a given dataset version.

        If `version="latest"`, the newest version will be used. If the specified
        version or its `meta.json` file does not exist, an empty dictionary is
        returned instead of raising.

        Parameters
        ----------
        name : str
            Dataset name.
        version : str, optional
            Version identifier, or "latest" (default) to use the newest one.

        Returns
        -------
        Dict
            Parsed metadata stored in `meta.json`, or an empty dict if the file
            cannot be found.
        """

        if version == "latest":
            versions = self.list_versions(name)
            if not versions:
                return {}
            version = versions[-1]
        mfile = self._ver_dir(name, version) / "meta.json"
        if not mfile.exists():
            return {}
        return json.loads(mfile.read_text(encoding="utf-8"))

    # ---------- 儲存（新版本） ----------
    def save(
        self,
        name: str,
        df: pd.DataFrame,
        meta: Optional[Dict] = None,
        fmt: Literal["parquet", "csv"] = "parquet"
    ) -> str:
        """
        Save a DataFrame as a new dataset version, with deduplication by content hash.

        If the newest existing version has the same content hash as the DataFrame
        being saved, no new version directory is created. Instead, the existing
        `meta.json` is updated (e.g. with extra keys and `updated_at`).

        If the content is different, a new version directory is created with:
        - `data.parquet` (or `data.csv` as a fallback or when explicitly requested)
        - `meta.json` containing basic information and the hash.

        Parameters
        ----------
        name : str
            Dataset name.
        df : pd.DataFrame
            DataFrame to be saved as this dataset version.
        meta : Dict, optional
            Additional metadata to be merged into `meta.json`.
        fmt : {"parquet", "csv"}, optional
            Preferred storage format. Defaults to "parquet". When "parquet" is
            requested but parquet writing fails, a CSV fallback is used and the
            reason is recorded in metadata.

        Returns
        -------
        str
            The version identifier that was used, either an existing one (when
            content is identical) or a newly created version.
        """

        dataset_dir = self._ds_dir(name)
        dataset_dir.mkdir(parents=True, exist_ok=True)

        new_hash = self._hash_df(df)
        latest_dir = self._latest_version(name) 
        # === 檢查最近版本是否相同 ===
        if latest_dir and (latest_dir / "meta.json").exists():
            with open(latest_dir / "meta.json", "r", encoding="utf-8") as f:
                last_meta = json.load(f)
            last_hash = last_meta.get("hash")

            if last_hash == new_hash:
                # 相同 → 更新 meta
                print(f"檢測到內容相同，更新 meta：{name}/{last_meta['version']}")
                last_meta.update(meta or {})
                last_meta["updated_at"] = datetime.now().isoformat(timespec="seconds")
                (latest_dir / "meta.json").write_text(
                    json.dumps(last_meta, ensure_ascii=False, indent=2), encoding="utf-8"
                )
                return last_meta["version"]

        # === 不同 → 建立新版本 ===
        version = self._now_ver()
        vdir = self._ver_dir(name, version)
        vdir.mkdir(parents=True, exist_ok=False)

        # === 原子寫入資料 ===
        if fmt == "parquet":
            try:
                tmp = vdir / "data.parquet.tmp"
                target = vdir / "data.parquet"
                df.to_parquet(tmp, index=False)
                os.replace(tmp, target)
            except Exception as e:
                # fallback
                tmp = vdir / "data.csv.tmp"
                target = vdir / "data.csv"
                df.to_csv(tmp, index=False)
                os.replace(tmp, target)
                meta = meta or {}
                meta["format_fallback"] = f"csv_due_to_{type(e).__name__}"

        elif fmt == "csv":
            tmp = vdir / "data.csv.tmp"
            target = vdir / "data.csv"
            df.to_csv(tmp, index=False)
            os.replace(tmp, target)
        else:
            raise ValueError("fmt must be 'parquet' or 'csv'.")

        # === 寫入 meta.json ===
        meta = meta or {}
        meta.update({
            "name": name,
            "version": version,
            "rows": int(df.shape[0]),
            "cols": int(df.shape[1]),
            "saved_at": datetime.now().isoformat(timespec="seconds"),
            "hash": new_hash
        })
        (vdir / "meta.json").write_text(
            json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
        )

        print(f" 新版本已儲存：{name}/{version}")
        return version

    # ---------- 更新既有版本（可只改 meta） ----------
    def update(
        self,
        name: str,
        version: str,
        df: Optional[pd.DataFrame] = None,
        meta: Optional[Dict] = None,
        fmt: Optional[Literal["parquet", "csv"]] = None
    ) -> None:
        """
        Update an existing dataset version, optionally replacing data and/or metadata.

        This method allows in-place modification of a specific version:
        - If `df` is provided, the underlying data file is rewritten (parquet or csv).
        Existing alternative format files are removed to keep a single source of truth.
        Metadata fields such as `rows`, `cols`, and `updated_at` are also refreshed.
        - If only `meta` is provided, the existing metadata is merged and updated
        without touching the data file.

        Parameters
        ----------
        name : str
            Dataset name.
        version : str
            Version identifier to update.
        df : pd.DataFrame, optional
            New data to replace the existing dataset content for this version.
        meta : Dict, optional
            Extra metadata to merge into the existing `meta.json`.
        fmt : {"parquet", "csv"}, optional
            Explicit storage format when rewriting data. If omitted and data is
            updated, the format is inferred from existing files, defaulting to
            "parquet" when nothing exists.

        Raises
        ------
        FileNotFoundError
            If the specified version directory does not exist.
        """

        vdir = self._ver_dir(name, version)
        if not vdir.exists():
            raise FileNotFoundError(f"Version '{version}' not found for '{name}'.")

        # 更新資料
        if df is not None:
            # 若未指定 fmt，推斷現有檔案
            if fmt is None:
                if (vdir / "data.parquet").exists():
                    fmt = "parquet"
                elif (vdir / "data.csv").exists():
                    fmt = "csv"
                else:
                    fmt = "parquet"  # 預設
            if fmt == "parquet":
                tmp = vdir / "data.parquet.tmp"
                target = vdir / "data.parquet"
                df.to_parquet(tmp, index=False)
                os.replace(tmp, target)
                # 若原本有 csv，清掉
                if (vdir / "data.csv").exists():
                    (vdir / "data.csv").unlink()
            else:
                tmp = vdir / "data.csv.tmp"
                target = vdir / "data.csv"
                df.to_csv(tmp, index=False)
                os.replace(tmp, target)
                if (vdir / "data.parquet").exists():
                    (vdir / "data.parquet").unlink()

            # 同步更新 meta 的 rows/cols
            existing_meta = self.load_meta(name, version)
            existing_meta["rows"] = int(df.shape[0])
            existing_meta["cols"] = int(df.shape[1])
            existing_meta["updated_at"] = datetime.now().isoformat(timespec="seconds")
            if meta:
                existing_meta.update(meta)
            (vdir / "meta.json").write_text(json.dumps(existing_meta, ensure_ascii=False, indent=2), encoding="utf-8")
        elif meta is not None:
            # 只更新 meta
            existing_meta = self.load_meta(name, version)
            existing_meta.update(meta)
            existing_meta["updated_at"] = datetime.now().isoformat(timespec="seconds")
            (vdir / "meta.json").write_text(json.dumps(existing_meta, ensure_ascii=False, indent=2), encoding="utf-8")

    # ---------- 刪除 ----------
    def delete(self, name: str, version: Optional[str] = None) -> None:
        """
        version=None → 整個 dataset 全刪
        version=指定 → 刪該版本
        """
        if version is None:
            d = self._ds_dir(name)
            if d.exists():
                shutil.rmtree(d)
        else:
            vdir = self._ver_dir(name, version)
            if vdir.exists():
                shutil.rmtree(vdir)

    # ---------- 保留最近 N 版 ----------
    def prune(self, name: str, keep: int = 3) -> List[str]:
        """
        Delete older versions of a dataset, keeping only the most recent ones.

        Versions are determined via `list_versions` (lexicographically sorted).
        All but the newest `keep` versions are removed.

        Parameters
        ----------
        name : str
            Dataset name to prune.
        keep : int, optional
            Number of newest versions to retain. Defaults to 3.

        Returns
        -------
        List[str]
            A list of version identifiers that were deleted. If no pruning was
            necessary, returns an empty list.
        """

        versions = self.list_versions(name)
        if len(versions) <= keep:
            return []
        to_delete = versions[:-keep]
        for v in to_delete:
            self.delete(name, v)
        return to_delete


# ===== 你的 config 包裝（可選，讓你用 config.data 管理倉庫） =====
class config:
    """
    Lightweight configuration namespace for data-related utilities.

    This class groups together shared configuration values (such as the base
    data directory) and exposes a nested `data` helper for convenient access
    to the `DatasetStore` API.
    """

    data_base = "./.data"

    class data:
        dataset_dir = "dataset"

        def __init__(self, root: Optional[str] = None):
            """
            Initialize a data helper bound to a specific base directory.

            This creates an underlying `DatasetStore` instance that all helper
            methods delegate to, allowing callers to use `config.data(...)` as a
            simple facade over the versioned dataset repository.

            Parameters
            ----------
            root : str, optional
                Root directory to store datasets under. If omitted, the global
                `config.data_base` value is used.
            """

            base = root or config.data_base
            self.store = DatasetStore(base)

        # 快捷方法
        def save(self, name: str, df: pd.DataFrame, meta: Optional[Dict] = None, fmt: str = "parquet") -> str:
            """
            Save a DataFrame into the versioned dataset store.

            This is a convenience wrapper around `DatasetStore.save`, allowing
            callers to use `config.data(...).save(...)` instead of interacting
            with the store directly.

            Parameters
            ----------
            name : str
                Dataset name.
            df : pd.DataFrame
                DataFrame to be stored.
            meta : Dict, optional
                Additional metadata to record along with the dataset.
            fmt : str, optional
                Storage format, typically "parquet" (default) or "csv".

            Returns
            -------
            str
                Version identifier returned by the underlying `DatasetStore.save`.
            """

            return self.store.save(name, df, meta=meta, fmt=fmt)

        def list(self, name: str) -> List[str]:
            """
            List all versions for the specified dataset.

            Parameters
            ----------
            name : str
                Dataset name.

            Returns
            -------
            List[str]
                Sorted list of version identifiers available for this dataset.
            """

            return self.store.list_versions(name)

        def load(self, name: str, version: str = "latest") -> pd.DataFrame:
            """
            Load a dataset as a pandas DataFrame via the underlying store.

            Parameters
            ----------
            name : str
                Dataset name.
            version : str, optional
                Version identifier to load, or "latest" (default) to use the newest one.

            Returns
            -------
            pd.DataFrame
                The loaded dataset.
            """

            return self.store.load(name, version)

        def load_meta(self, name: str, version: str = "latest") -> Dict:
            """
            Load metadata for a dataset version via the underlying store.

            Parameters
            ----------
            name : str
                Dataset name.
            version : str, optional
                Version identifier, or "latest" (default) to use the newest one.

            Returns
            -------
            Dict
                Metadata dictionary stored in `meta.json`, or an empty dict when
                no metadata is available.
            """

            return self.store.load_meta(name, version)

        def update(self, name: str, version: str, df: Optional[pd.DataFrame] = None, meta: Optional[Dict] = None, fmt: Optional[str] = None):
            """
            Update an existing dataset version via the underlying store.

            This forwards all arguments to `DatasetStore.update`, allowing callers
            to modify data files, metadata, or both, for a specific version.

            Parameters
            ----------
            name : str
                Dataset name.
            version : str
                Version identifier to update.
            df : pd.DataFrame, optional
                New data to replace the existing content.
            meta : Dict, optional
                Metadata updates to merge into `meta.json`.
            fmt : str, optional
                Explicit storage format when rewriting data ("parquet" or "csv").
            """

            return self.store.update(name, version, df=df, meta=meta, fmt=fmt)

        def delete(self, name: str, version: Optional[str] = None):
            """
            Delete an entire dataset or a specific version via the underlying store.

            Parameters
            ----------
            name : str
                Dataset name.
            version : str, optional
                Version identifier to delete. If omitted, all versions and the
                dataset directory are removed.
            """

            return self.store.delete(name, version)

        def prune(self, name: str, keep: int = 3) -> List[str]:
            """
            Prune older versions of a dataset, keeping only the most recent ones.

            This is a thin wrapper around `DatasetStore.prune`, useful for housekeeping
            to prevent the version store from growing unbounded.

            Parameters
            ----------
            name : str
                Dataset name to prune.
            keep : int, optional
                Number of newest versions to retain. Defaults to 3.

            Returns
            -------
            List[str]
                A list of version identifiers that were deleted.
            """

            return self.store.prune(name, keep=keep)
